**Paper reproduction code record by wujing^*^**

## Knowledge Distillation

|          Model          |                            Paper                             |  Venue  | Year |                             Code                             | Time       |
| :---------------------: | :----------------------------------------------------------: | :-----: | :--: | :----------------------------------------------------------: | ---------- |
|         YOLOv7          | [YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors](https://arxiv.org/pdf/2207.02696v1) |  CVPR   | 2022 |        [Python](https://github.com/wongkinyiu/yolov7)        | 2022.05.22 |
|          SimKD          | [Knowledge Distillation with the Reused Teacher Classifier]( https://arxiv.org/abs/2203.14001) |  CVPR   | 2022 |        [Python](https://github.com/wongkinyiu/yolov7)        | 2023.06.22 |
|           DKD           | [Decoupled Knowledge Distillation](https://arxiv.org/abs/2203.08679) |  CVPR   | 2022 | [Python](https://github.com/megvii-research/mdistiller/releases/tag/checkpoints) | 2023.10.01 |
|      Good-DA-in-KD      | [What Makes a "Good" Data Augmentation in Knowledge Distillation -- A Statistical Perspective](https://arxiv.org/abs/2012.02909) | NeurIPS | 2022 |    [Python](https://mingsun-tse.github.io/Good-DA-in-KD/)    | 2023.11.20 |
|      RepDistiller       | [Contrastive Representation Distillation](http://arxiv.org/abs/1910.10699) |  ICLR   | 2020 |          [Python](http://hobbitlong.github.io/CRD/)          | 2023.7.24  |
|          CTKD           | [Curriculum Temperature for Knowledge Distillation](https://arxiv.org/abs/2211.16231) |  AAAI   | 2023 |         [Python](https://github.com/zhengli97/CTKD)          | 2024.01.24 |
| Two-stages Distillation | [Effective Whole-body Pose Estimation with Two-stages Distillation](https://arxiv.org/abs/2307.15880) |  ICCVW  | 2023 |      [Python](https://github.com/IDEA-Research/DWPose)       | 2024.02.25 |
